---
phase: 02-core-recording-transcription
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - OptionC/Audio/AudioCaptureManager.swift
  - OptionC/Transcription/TranscriptionEngine.swift
autonomous: true

must_haves:
  truths:
    - "Audio buffers are captured from microphone and streamed to speech recognizer"
    - "Speech recognizer produces transcription result from audio input"
    - "Transcription times out after 30 seconds of no final result"
  artifacts:
    - path: "OptionC/Audio/AudioCaptureManager.swift"
      provides: "AVAudioEngine microphone capture with buffer streaming"
      exports: ["AudioCaptureManager", "startCapture", "stopCapture"]
    - path: "OptionC/Transcription/TranscriptionEngine.swift"
      provides: "SFSpeechRecognizer transcription with timeout"
      exports: ["TranscriptionEngine", "transcribe", "cancel"]
  key_links:
    - from: "AudioCaptureManager"
      to: "SFSpeechAudioBufferRecognitionRequest"
      via: "buffer.append in installTap closure"
      pattern: "recognitionRequest.*append.*buffer"
    - from: "TranscriptionEngine"
      to: "SFSpeechRecognizer"
      via: "recognitionTask with request"
      pattern: "speechRecognizer.*recognitionTask"
---

<objective>
Build the audio capture and transcription foundation for voice-to-text.

Purpose: Establish the core infrastructure that captures microphone audio and converts it to text using Apple's Speech framework. This is the heart of Option-C's functionality.

Output: Two Swift classes that handle the complete audio-to-text pipeline: AudioCaptureManager for microphone capture and TranscriptionEngine for speech recognition.
</objective>

<execution_context>
@/Users/connecteddale/.claude/get-shit-done/workflows/execute-plan.md
@/Users/connecteddale/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-recording-transcription/02-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AudioCaptureManager</name>
  <files>OptionC/Audio/AudioCaptureManager.swift</files>
  <action>
Create AudioCaptureManager class that handles microphone audio capture using AVAudioEngine.

Implementation requirements:
1. Class annotated with @MainActor for thread safety
2. Use Optional AVAudioEngine (var audioEngine: AVAudioEngine?) - create fresh instance per session
3. Hold reference to SFSpeechAudioBufferRecognitionRequest for buffer streaming
4. Implement startCapture(request: SFSpeechAudioBufferRecognitionRequest) method:
   - Create fresh AVAudioEngine()
   - Get inputNode from engine
   - Get recordingFormat from inputNode.outputFormat(forBus: 0)
   - Install tap with bufferSize: 1024, format: recordingFormat
   - In tap closure: request.append(buffer)
   - Call audioEngine.prepare() then try audioEngine.start()
5. Implement stopCapture() method:
   - Call audioEngine?.inputNode.removeTap(onBus: 0)
   - Call audioEngine?.stop()
   - Set audioEngine = nil for complete deallocation

Critical pattern from research: Create fresh AVAudioEngine instance for each recording session. Reusing instances causes state corruption where tap callback stops firing.

Use [weak self] in tap closure to prevent retain cycles. Add isCapturing computed property based on audioEngine?.isRunning.
  </action>
  <verify>
File exists at OptionC/Audio/AudioCaptureManager.swift with:
- @MainActor annotation on class
- Optional audioEngine property
- startCapture method that creates fresh engine
- stopCapture method that nils out engine
- installTap call with bufferSize: 1024
  </verify>
  <done>
AudioCaptureManager compiles and has all required methods for microphone capture with buffer streaming to speech recognition request.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create TranscriptionEngine</name>
  <files>OptionC/Transcription/TranscriptionEngine.swift</files>
  <action>
Create TranscriptionEngine class that handles speech-to-text using SFSpeechRecognizer.

Implementation requirements:
1. Class annotated with @MainActor for thread safety
2. Private speechRecognizer: SFSpeechRecognizer? initialized with Locale.current
3. Private recognitionTask: SFSpeechRecognitionTask? for tracking current task
4. Private timeoutTimer: Timer? for handling unresponsive transcription
5. Define TranscriptionError enum: timeout, recognizerUnavailable, noSpeechDetected

Implement transcribe method:
```swift
func transcribe(
    request: SFSpeechAudioBufferRecognitionRequest,
    timeout: TimeInterval = 30.0,
    onPartialResult: ((String) -> Void)? = nil,
    completion: @escaping (Result<String, Error>) -> Void
)
```

Method logic:
- Guard speechRecognizer != nil and isAvailable, else completion(.failure(.recognizerUnavailable))
- Configure request: shouldReportPartialResults = true, requiresOnDeviceRecognition = true (offline)
- Start timeout timer that calls cancel() and completion(.failure(.timeout))
- Create recognitionTask = speechRecognizer.recognitionTask(with: request) { result, error in }
- In callback: invalidate timeout timer on any result
- If error: completion(.failure(error)), return
- If result.isFinal: completion(.success(result.bestTranscription.formattedString))
- Else if onPartialResult != nil: call with partial result

Implement cancel() method:
- Invalidate and nil timeout timer
- Cancel and nil recognition task

Critical pattern from research: Always call request.endAudio() before stopping - this is done by the caller (RecordingController), not here. TranscriptionEngine just processes whatever audio it receives.
  </action>
  <verify>
File exists at OptionC/Transcription/TranscriptionEngine.swift with:
- @MainActor annotation on class
- SFSpeechRecognizer initialized with Locale.current
- TranscriptionError enum with timeout case
- transcribe method with timeout parameter (default 30.0)
- requiresOnDeviceRecognition = true for offline support
- cancel method that cleans up timer and task
  </verify>
  <done>
TranscriptionEngine compiles and provides async transcription with timeout handling and offline-only recognition.
  </done>
</task>

</tasks>

<verification>
1. Both files compile without errors when added to Xcode project
2. AudioCaptureManager.startCapture creates new AVAudioEngine instance
3. TranscriptionEngine.transcribe sets requiresOnDeviceRecognition = true
4. Timeout timer fires after 30 seconds if no final result
5. No retain cycles (weak self in closures)
</verification>

<success_criteria>
- AudioCaptureManager can capture microphone audio and stream to recognition request
- TranscriptionEngine can process audio request and return transcribed text
- Offline transcription enforced via requiresOnDeviceRecognition
- 30-second timeout prevents infinite waits
- Clean resource management (fresh engine per session, timer cleanup)
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-recording-transcription/02-01-SUMMARY.md`
</output>
